{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple, Union, Optional, Any\n",
    "from env import ConnectFourEnv\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from replay_buffer import ReplayBuffer\n",
    "from monte_carlo import MonteCarloTreeSearchAgent \n",
    "from agent import RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [03:37<08:31, 36.53s/it]"
     ]
    }
   ],
   "source": [
    "env = ConnectFourEnv()\n",
    "env.step(0)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(3)\n",
    "env.step(1)\n",
    "# env.render()\n",
    "monte = MonteCarloTreeSearchAgent(env, n_iterations=10000, c=1.4)\n",
    "# monte.choose_action()\n",
    "\n",
    "\n",
    "\n",
    "env.play(monte, RandomAgent(env), n_games=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape[0] * input_shape[1], 42),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(42, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent(Agent):\n",
    "    def __init__(self, env, replay_buffer, evaluation_agent=None):\n",
    "        super().__init__(env)\n",
    "        self.name = \"DQNAgent\"\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.replay_buffer: ReplayBuffer = replay_buffer\n",
    "        self.policy_net = DQN(self.state_dim, self.action_dim)\n",
    "        self.target_net = DQN(self.state_dim, self.action_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Target net is not trained\n",
    "        self.lr = 0.005\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        self.steps_done = 0\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.15 # if you leave overnight, you can decrease this to 0.01\n",
    "        self.epsilon_decay = 500_000 # if you leave overnight, you can increase this to 1_000_000\n",
    "        self.batch_size = 256\n",
    "        self.gamma = 1  # Discount factor\n",
    "        self.target_update = 1000\n",
    "        if evaluation_agent is None:\n",
    "            self.evaluation_agent = RandomAgent(self.env)\n",
    "        else:\n",
    "            self.evaluation_agent = evaluation_agent\n",
    "\n",
    "    def load_trained_model_from_file(self, path):\n",
    "        self.policy_net = torch.load(path)\n",
    "        self.policy_net.eval()\n",
    "\n",
    "    def choose_action(self, explore=False) -> int:\n",
    "        sample = random.random()\n",
    "        epsilon_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "            math.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        if sample > epsilon_threshold or not explore:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(\n",
    "                    self.env.board, dtype=torch.float).unsqueeze(0)\n",
    "                decision = self.policy_net(state)\n",
    "                return decision.max(1)[1].view(1, 1).item()\n",
    "        else:\n",
    "            return random.randrange(self.action_dim)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample a batch of experiences from the replay buffer\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Separate the components of each transition\n",
    "        batch_states = torch.stack(batch.state).float()\n",
    "        batch_actions = torch.stack(batch.action).view(-1, 1).long()\n",
    "        batch_rewards = torch.tensor(batch.reward, dtype=torch.float)\n",
    "        batch_next_states = torch.stack(batch.next_state).float()\n",
    "        batch_dones = torch.tensor(batch.done, dtype=torch.float)\n",
    "\n",
    "        # Calculate current Q-values from the policy_net\n",
    "        current_q_values = self.policy_net(batch_states).gather(\n",
    "            1, batch_actions).squeeze(1)\n",
    "\n",
    "        # Calculate the maximum Q-value for the next states from the target_net\n",
    "        next_state_values = self.target_net(\n",
    "            batch_next_states).max(1)[0].detach()\n",
    "        \n",
    "\n",
    "        # Apply (1 - done) to zero out the values for terminal states\n",
    "        next_state_values = next_state_values * (1 - batch_dones)\n",
    "\n",
    "        # Compute the expected Q values for the current state-action pairs\n",
    "        expected_q_values = batch_rewards + self.gamma * next_state_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(current_q_values, expected_q_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, n_games, print_interval):\n",
    "        losses = []\n",
    "\n",
    "        self.evaluate(50, message=True)\n",
    "\n",
    "        for episode in tqdm(range(n_games)):\n",
    "            observation = self.env.reset()\n",
    "            observation = torch.tensor(observation, dtype=torch.float)\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.choose_action(explore=True)\n",
    "                self.steps_done += 1\n",
    "\n",
    "                next_observation, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                # the agent thinks he is player 1, so we need to flip the board and the player\n",
    "                self.env.flip_board()\n",
    "                self.env.current_player = 3 - self.env.current_player\n",
    "                \n",
    "                next_observation = torch.tensor(\n",
    "                    next_observation, dtype=torch.float)\n",
    "\n",
    "                reward = torch.tensor([reward], dtype=torch.float)\n",
    "                done_tensor = torch.tensor([done], dtype=torch.float)\n",
    "\n",
    "                self.replay_buffer.push(\n",
    "                    observation, action, reward, next_observation, done_tensor)\n",
    "                observation = next_observation\n",
    "\n",
    "                loss = self.optimize_model()\n",
    "\n",
    "                if loss is not None:\n",
    "                    losses.append(loss)\n",
    "\n",
    "            if (episode + 1) % self.target_update == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            if (episode + 1) % print_interval == 0 and len(losses) > 0:\n",
    "                avg_loss = sum(losses[-print_interval:]) / len(losses[-print_interval:])\n",
    "                print(f\"Episode {episode + 1}: Average Loss = {avg_loss}\")\n",
    "                self.evaluate(50, message=True)\n",
    "                self.evaluate(50, message=True, evaluation_agent=RandomAgent(self.env))\n",
    "                print()\n",
    "                torch.save(self.policy_net, f\"checkpoints/model_{episode + 1}.pt\")\n",
    "               \n",
    "\n",
    "    def evaluate(self, n_games, show=False, message=False, evaluation_agent=None):\n",
    "        if evaluation_agent is None:\n",
    "            evaluation_agent = self.evaluation_agent\n",
    "\n",
    "        wins, avg_length = self.env.play(self, evaluation_agent, n_games, show)\n",
    "        if message:\n",
    "            print(f\"Out of {n_games} games against {evaluation_agent.name}, the model won {wins[1]} games : {wins[1] / n_games * 100:.2f}% with an average game length of {avg_length}\")\n",
    "        return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model\n",
    "env = ConnectFourEnv()\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "agent = DQNAgent(env, replay_buffer)\n",
    "# agent.load_trained_model_from_file(\"checkpoints/model_6000.pt\")\n",
    "agent.evaluation_agent = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "agent.train(1_000_000, print_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play(RandomAgent(env), RandomAgent(env), n_games=1, show_outcome=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play(agent, agent, n_games=1, show_game=True, show_outcome=True)\n",
    "# env.play(agent, agent, n_games=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def choose_action(self):\n",
    "        action = int(input(\"Enter your move: \"))\n",
    "        while action not in range(self.env.columns) or self.env.playable_rows[action] == -1:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            action = int(input(\"Enter your move: \"))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play against the trained model\n",
    "\n",
    "env.play(agent, HumanAgent(env), n_games=1, show_game=True, show_outcome=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(agent.policy_net, 'connect_four_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume training\n",
    "\n",
    "env = ConnectFourEnv()\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "agent = DQNAgent(env, replay_buffer)\n",
    "agent.policy_net = torch.load('connect_four_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.evaluate(1000, message=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import profilers and check the training bottlenecks of the model\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    agent.train(50, print_interval=100)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
